{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Build a RAG Example with TiDB, DSPy, Meta Llama 3, Amazon Bedrock and Boto3\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook we will show you how to use TiDB and Boto3 SDK to build a RAG with Meta Llama 3 and Amazon Bedrock. [TiDB](https://tidb.cloud/?utm_source=github&utm_medium=community&utm_campaign=video_aws_example_generativeai_cm_0624) is an open-source distributed SQL database that supports Hybrid Transactional and Analytical Processing (HTAP) workloads. It is MySQL compatible and features horizontal scalability, strong consistency, and high availability. You can deploy TiDB in a self-hosted environment or in the cloud.\n",
    "\n",
    "### Use case\n",
    "\n",
    "To demonstrate the vector search capability of TiDB Serverless, and the text generation capability of Meta Llama 3, let's take the use case of build a RAG Q&A bot.\n",
    "\n",
    "### Persona\n",
    "\n",
    "You are a TiDB user, you want to build a searching engine about TiDB. So you can build a RAG Q&A bot to achieve it.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "To fulfill this use case, in this notebook we will show how to build a RAG application. Save the information to TiDB Serverless, use vector search feature in TiDB Serverless to get information. And then, we will use those information to generate the answer via Meta Llama 3. We will use the TiDB, and the Meta Llama 3 model through the Amazon Bedrock API with Boto3 SDK."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Python 3.10\n",
    "\n",
    "⚠  For this lab we need to run the notebook based on a Python 3.10 runtime. ⚠"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setting\n",
    "\n",
    "Before you run this Jupyter Notebook, please set the environment variables:\n",
    "\n",
    "- TIDB_HOST\n",
    "- TIDB_PORT\n",
    "- TIDB_USER\n",
    "- TIDB_PASSWORD\n",
    "- TIDB_DB_NAME\n",
    "\n",
    "\n",
    "> **Warning:**\n",
    ">\n",
    "> Aware that this notebook will drop some tables and recreate them, please use a new TiDB Serverless cluster."
   ],
   "metadata": {
    "id": "xv-ufMosI466"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Installation\n",
    "\n",
    "To run this notebook you would need to install dependencies - SQLAlchemy, tidb-vector, DSPy, Langchain Community, PyMySQL, pydantic, boto3, pyvis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "%pip install PyMySQL==1.1.0 --force-reinstall --quiet\n",
    "%pip install SQLAlchemy==2.0.30 --force-reinstall --quiet\n",
    "%pip install tidb-vector==0.0.9 --force-reinstall --quiet\n",
    "%pip install pydantic==2.7.1 --force-reinstall --quiet\n",
    "%pip install pydantic_core==2.18.2 --force-reinstall --quiet\n",
    "%pip install boto3 --force-reinstall --quiet"
   ],
   "metadata": {
    "id": "J1twp_Mabzuw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Kernel Restart\n",
    "\n",
    "Restart the kernel with the updated packages that are installed through the dependencies above"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "Import the necessary libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import boto3\n",
    "\n",
    "from sqlalchemy import (\n",
    "    Column,\n",
    "    Integer,\n",
    "    Text,\n",
    "    URL,\n",
    "    create_engine,\n",
    ")\n",
    "from sqlalchemy.orm import Session, declarative_base\n",
    "from tidb_vector.sqlalchemy import VectorType"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initialization\n",
    "\n",
    "Connect to a TiDB Cloud Cluster and Initiate Bedrock Runtime Client"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_db_url():\n",
    "    return URL(\n",
    "        drivername=\"mysql+pymysql\",\n",
    "        username=os.environ[\"TIDB_USER\"],\n",
    "        password=os.environ[\"TIDB_PASSWORD\"],\n",
    "        host=os.environ['TIDB_HOST'],\n",
    "        port=int(os.environ[\"TIDB_PORT\"]),\n",
    "        database=os.environ[\"TIDB_DB_NAME\"],\n",
    "        query={\"ssl_verify_cert\": True, \"ssl_verify_identity\": True},\n",
    "    )\n",
    "\n",
    "engine = create_engine(get_db_url(), pool_recycle=300)\n",
    "bedrock_runtime = boto3.client('bedrock-runtime')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model invocation\n",
    "\n",
    "Invoke the Amazon Titan Text Embeddings V2 model using bedrock runtime client\n",
    "\n",
    "Amazon Bedrock runtime client provides you with an API `invoke_model` which accepts the following:\n",
    "- `modelId`: This is the model ARN for the foundation model available in Amazon Bedrock\n",
    "- `accept`: The type of input request\n",
    "- `contentType`: The content type of the output\n",
    "- `body`: A json string payload consisting of the prompt and the configurations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embedding_model_name = \"amazon.titan-embed-text-v2:0\"\n",
    "dim_of_embedding_model = 512\n",
    "llm_name = \"meta.llama3-70b-instruct-v1:0\"\n",
    "\n",
    "def embedding(content):\n",
    "    payload = {\n",
    "        \"modelId\": embedding_model_name,\n",
    "        \"contentType\": \"application/json\",\n",
    "        \"accept\": \"*/*\",\n",
    "        \"body\": {\n",
    "            \"inputText\": content,\n",
    "            \"dimensions\": dim_of_embedding_model,\n",
    "            \"normalize\": True,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Convert the payload to bytes\n",
    "    body_bytes = json.dumps(payload['body']).encode('utf-8')\n",
    "\n",
    "    # Invoke the model\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        body=body_bytes,\n",
    "        contentType=payload['contentType'],\n",
    "        accept=payload['accept'],\n",
    "        modelId=payload['modelId']\n",
    "    )\n",
    "\n",
    "    result_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "    return result_body.get(\"embedding\")\n",
    "\n",
    "def generate_result(query: str, info_str: str):\n",
    "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "ONLY use the content below to generate an answer:\n",
    "{info_str}<|eot_id|>\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Please carefully think about the question: {query}<|eot_id|>\n",
    "\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "    payload = {\n",
    "        \"modelId\": llm_name,\n",
    "        \"contentType\": \"application/json\",\n",
    "        \"accept\": \"application/json\",\n",
    "        \"body\": {\n",
    "            \"prompt\": prompt,\n",
    "            \"temperature\": 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Convert the payload to bytes\n",
    "    body_bytes = json.dumps(payload['body']).encode('utf-8')\n",
    "\n",
    "    # Invoke the model\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        body=body_bytes,\n",
    "        contentType=payload['contentType'],\n",
    "        accept=payload['accept'],\n",
    "        modelId=payload['modelId']\n",
    "    )\n",
    "\n",
    "    result_body = json.loads(response.get(\"body\").read())\n",
    "    completion = result_body[\"generation\"]\n",
    "    return completion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TiDB Table and Vector Index\n",
    "\n",
    "Create table and its vector index in TiDB Serverless to store text and vector."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Base = declarative_base()\n",
    "class Entity(Base):\n",
    "    __tablename__ = \"entity\"\n",
    "\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    content = Column(Text)\n",
    "    content_vec = Column(\n",
    "        VectorType(dim=dim_of_embedding_model),\n",
    "        comment=\"hnsw(distance=l2)\"\n",
    "    )\n",
    "\n",
    "Base.metadata.create_all(engine)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save Vector to TiDB Serverless\n",
    "\n",
    "Save 5 records with embedding vector to TiDB Serverless."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tidb_content = 'TiDB is an open-source distributed SQL database that supports Hybrid Transactional and Analytical Processing (HTAP) workloads.'\n",
    "tikv_content = 'TiKV is an open-source, distributed, and transactional key-value database. Unlike other traditional NoSQL systems.'\n",
    "tiflash_content = 'TiFlash is the key component that makes TiDB essentially an Hybrid Transactional/Analytical Processing (HTAP) database. As a columnar storage extension of TiKV, TiFlash provides both good isolation level and strong consistency guarantee.'\n",
    "pd_content = 'The Placement Driver (PD) server is the metadata managing component of the entire cluster.'\n",
    "tidb_cloud_content = 'TiDB Cloud is a fully-managed Database-as-a-Service (DBaaS) that brings TiDB, an open-source Hybrid Transactional and Analytical Processing (HTAP) database, to your cloud. TiDB Cloud offers an easy way to deploy and manage databases to let you focus on your applications, not the complexities of the databases.'\n",
    "\n",
    "with Session(engine) as session:\n",
    "    session.add(Entity(content = tidb_content, content_vec = embedding(tidb_content)))\n",
    "    session.add(Entity(content = tikv_content, content_vec = embedding(tikv_content)))\n",
    "    session.add(Entity(content = tiflash_content, content_vec = embedding(tiflash_content)))\n",
    "    session.add(Entity(content = pd_content, content_vec = embedding(pd_content)))\n",
    "    session.add(Entity(content = tidb_cloud_content, content_vec = embedding(tidb_cloud_content)))\n",
    "    session.commit()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ask Question"
   ],
   "metadata": {
    "id": "aCAeZfZd992I"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "question = \"What is the relationship between TiKV and TiFlash?\" # @param {type:\"string\"}"
   ],
   "metadata": {
    "id": "DWw2lyLmEf-P",
    "cellView": "form"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Find Information by Vector Search\n",
    "\n",
    "In this case, we will get the nearest 3 contents, by using embedding vector which the feature offered by TiDB Serverless.\n"
   ],
   "metadata": {
    "id": "kcvp8wi9En-7"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "question_embedding = embedding(question)\n",
    "with Session(engine) as session:\n",
    "    info_list = session.query(Entity) \\\n",
    "        .order_by(Entity.content_vec.cosine_distance(question_embedding)) \\\n",
    "        .limit(3).all()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generate Answer\n",
    "\n",
    "Once we got the entities and relationships, we can generate the answer via the Meta Llama 3."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "info_str = '\\n'.join(map(lambda info: info.content, info_list))\n",
    "result = generate_result(question, info_str)\n",
    "result"
   ],
   "metadata": {
    "id": "yHyc9qFb-zQJ",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "outputId": "9a70eff1-5c8e-4180-ab51-9b199af73de9"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
