{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6ca1cf4-fb82-4a2c-bb7a-9c9f4f91a8d2",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) Chatbot using Meta LLMs and Amazon S3\n",
    "\n",
    "This notebook demonstrates how to build a simple Retrieval-Augmented Generation (RAG) chatbot using Meta's LLM via Amazon Bedrock. Source documents are stored in Amazon S3, and Amazon Titan is used to generate vector embeddings. The flow is orchestrated using LangChain, which handles document loading, embedding, and semantic retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7f9dc5-a851-4f04-8e93-b10047ee95ee",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb4371c2-02f3-4a15-9cd9-9767fd645dce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T10:35:50.412044Z",
     "iopub.status.busy": "2025-07-11T10:35:50.411794Z",
     "iopub.status.idle": "2025-07-11T10:35:55.616190Z",
     "shell.execute_reply": "2025-07-11T10:35:55.615338Z",
     "shell.execute_reply.started": "2025-07-11T10:35:50.412024Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install boto3 langchain faiss-cpu s3fs pymupdf --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fdad72-8edf-4344-8151-77162fbc9c3d",
   "metadata": {},
   "source": [
    "Import dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f00e7847-6121-404e-90a5-b61d7cd084b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T10:35:55.618302Z",
     "iopub.status.busy": "2025-07-11T10:35:55.617968Z",
     "iopub.status.idle": "2025-07-11T10:35:57.014482Z",
     "shell.execute_reply": "2025-07-11T10:35:57.013723Z",
     "shell.execute_reply.started": "2025-07-11T10:35:55.618267Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "from langchain_aws import ChatBedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5908bceb-d5f0-42e6-a3a0-223bd1186242",
   "metadata": {},
   "source": [
    "Setup constants "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9590e13-e323-4aeb-bda1-69bac9f032c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T10:35:57.015739Z",
     "iopub.status.busy": "2025-07-11T10:35:57.015322Z",
     "iopub.status.idle": "2025-07-11T10:35:57.020245Z",
     "shell.execute_reply": "2025-07-11T10:35:57.019607Z",
     "shell.execute_reply.started": "2025-07-11T10:35:57.015717Z"
    }
   },
   "outputs": [],
   "source": [
    "AWS_REGION = \"us-east-1\"\n",
    "S3_BUCKET = \"lior-llama3-rag-chatbot-data-20255\"\n",
    "EMBED_MODEL_ID = \"amazon.titan-embed-text-v1\"\n",
    "LLM_MODEL_ID = \"meta.llama3-70b-instruct-v1:0\"\n",
    "PDF_FILE = 'media/sample.pdf'\n",
    "PDF_S3_KEY = 'media/sample.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eab237-505e-46f6-bdb1-413593b7dafe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T15:09:10.734843Z",
     "iopub.status.busy": "2025-07-06T15:09:10.734455Z",
     "iopub.status.idle": "2025-07-06T15:09:10.942716Z",
     "shell.execute_reply": "2025-07-06T15:09:10.942003Z",
     "shell.execute_reply.started": "2025-07-06T15:09:10.734814Z"
    }
   },
   "source": [
    "### Setup S3\n",
    "\n",
    "As a demostration, we upload the local PDF to S3 in order to demostrated a full S3 integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c44efed-565d-4e9e-aef7-18418c8919ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T10:35:57.023145Z",
     "iopub.status.busy": "2025-07-11T10:35:57.021593Z",
     "iopub.status.idle": "2025-07-11T10:35:57.360694Z",
     "shell.execute_reply": "2025-07-11T10:35:57.359061Z",
     "shell.execute_reply.started": "2025-07-11T10:35:57.023105Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 'lior-llama3-rag-chatbot-data-20255' exists.\n",
      "Uploaded media/sample.pdf to s3://lior-llama3-rag-chatbot-data-20255/media/sample.pdf\n"
     ]
    }
   ],
   "source": [
    "from s3_utils import upload_file_to_s3\n",
    "\n",
    "upload_file_to_s3(\n",
    "    pdf_file=PDF_FILE,\n",
    "    bucket=S3_BUCKET,\n",
    "    key=PDF_S3_KEY,\n",
    "    region=AWS_REGION\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d328c87d-d840-48f2-986e-43f49cd41b13",
   "metadata": {},
   "source": [
    "## Load Documents from Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c85614a-ebf3-436f-a89c-1e02c1592383",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T10:35:57.362353Z",
     "iopub.status.busy": "2025-07-11T10:35:57.362095Z",
     "iopub.status.idle": "2025-07-11T10:35:57.563480Z",
     "shell.execute_reply": "2025-07-11T10:35:57.562717Z",
     "shell.execute_reply.started": "2025-07-11T10:35:57.362330Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded PDF from S3 to media/sample.pdf\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "import fitz\n",
    "\n",
    "s3 = boto3.client(\"s3\", region_name=AWS_REGION)\n",
    "s3.download_file(S3_BUCKET, PDF_S3_KEY, PDF_FILE)\n",
    "print(f\"Downloaded PDF from S3 to {PDF_FILE}\")\n",
    "\n",
    "doc_text = \"\"\n",
    "with fitz.open(PDF_FILE) as doc:\n",
    "    for page in doc:\n",
    "        doc_text += page.get_text()\n",
    "\n",
    "# --- Wrap into LangChain Document\n",
    "documents = [Document(page_content=doc_text, metadata={\"source\": \"sample.pdf\"})]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2466c257-9377-49d1-9596-5243d6f86657",
   "metadata": {},
   "source": [
    "## Chunk and Embed Documents\n",
    "\n",
    "Split documents into manageable chunks and embed them using Amazon Titan Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d8ab954-0f69-41a7-b73c-78915c224a5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T10:42:32.141238Z",
     "iopub.status.busy": "2025-07-11T10:42:32.140720Z",
     "iopub.status.idle": "2025-07-11T10:42:37.096700Z",
     "shell.execute_reply": "2025-07-11T10:42:37.096128Z",
     "shell.execute_reply.started": "2025-07-11T10:42:32.141207Z"
    }
   },
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "texts = splitter.split_documents(documents)\n",
    "\n",
    "embedding = BedrockEmbeddings(model_id=EMBED_MODEL_ID)\n",
    "vectordb = FAISS.from_documents(texts, embedding)\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0318ffa-0fa4-4ca8-863e-38e9f350ec6f",
   "metadata": {},
   "source": [
    "## Build Meta-Powered RAG Chain\n",
    "\n",
    "We use Meta's Llama 3 model via Bedrock to power the chatbot with retrieval-augmented context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22662901-1a35-4af4-b77b-1b1bcc3623a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T10:42:54.674533Z",
     "iopub.status.busy": "2025-07-11T10:42:54.673823Z",
     "iopub.status.idle": "2025-07-11T10:42:54.683520Z",
     "shell.execute_reply": "2025-07-11T10:42:54.682712Z",
     "shell.execute_reply.started": "2025-07-11T10:42:54.674503Z"
    }
   },
   "outputs": [],
   "source": [
    "llm = ChatBedrock(model_id=LLM_MODEL_ID)\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type=\"stuff\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5d38dd-0e04-47d2-8871-c9b2bd683aa7",
   "metadata": {},
   "source": [
    "## Chat\n",
    "Ask a question about the document that was embbeded, see the answer and the chunks that were used in order to generate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b5448a7-ac05-41ac-9bb0-60c399d23014",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T10:42:58.692111Z",
     "iopub.status.busy": "2025-07-11T10:42:58.691703Z",
     "iopub.status.idle": "2025-07-11T10:42:59.849674Z",
     "shell.execute_reply": "2025-07-11T10:42:59.848521Z",
     "shell.execute_reply.started": "2025-07-11T10:42:58.692086Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      "Alexander Adell is a character in a science fiction story. He is one of the two main characters, along with Lupov, and is involved in the maintenance of a giant computer called Multivac.\n",
      "\n",
      "Retrieved Chunks:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "of each other and the bottle. \n",
      " \n",
      "\"It's amazing when you think of it,\" said Adell. His broad face had lines of weariness in it, and he stirred \n",
      "his drink slowly with a glass rod, watching the cubes of ice slur clumsily about. \"All the energy we can \n",
      "possibly ever use for free. Enough energy, if we wanted to draw on it, to melt all Earth into a big drop of \n",
      "impure liquid iron, and still never miss the energy so used. All the energy we could ever use, forever and \n",
      "forever and forever.\"\n",
      "\n",
      "--- Chunk 2 ---\n",
      "that giant computer. They had at least a vague notion of the general plan of relays and circuits that had \n",
      "long since grown past the point where any single human could possibly have a firm grasp of the whole. \n",
      " \n",
      "Multivac was self-adjusting and self-correcting. It had to be, for nothing human could adjust and correct it \n",
      "quickly enough or even adequately enough -- so Adell and Lupov attended the monstrous giant only\n",
      "\n",
      "--- Chunk 3 ---\n",
      "to another sun.\" \n",
      " \n",
      "There was silence for a while. Adell put his glass to his lips only occasionally, and Lupov's eyes slowly \n",
      "closed. They rested. \n",
      " \n",
      "Then Lupov's eyes snapped open. \"You're thinking we'll switch to another sun when ours is done, aren't \n",
      "you?\" \n",
      " \n",
      "\"I'm not thinking.\" \n",
      " \n",
      "\"Sure you are. You're weak on logic, that's the trouble with you. You're like the guy in the story who was \n",
      "caught in a sudden shower and Who ran to a grove of trees and got under one. He wasn't worried, you\n"
     ]
    }
   ],
   "source": [
    "query = \"Who is Alexander Adell?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "# Print the answer\n",
    "print(\"\\nAnswer:\")\n",
    "print(result[\"result\"])\n",
    "\n",
    "# Print the retrieved chunks used for the answer\n",
    "print(\"\\nRetrieved Chunks:\")\n",
    "for i, doc in enumerate(result[\"source_documents\"]):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    print(doc.page_content.strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
